{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce1e942-0329-4d4d-9227-361ac5e7cc99",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f7c033-3efd-45c8-b386-50bde05b3eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.5 environment at: /opt/python\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m52 packages\u001b[0m \u001b[2min 488ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m4 packages\u001b[0m \u001b[2min 284ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/28] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K██░░░░░░░░░░░░░░░░░░ [3/28] \u001b[2mpyngrok==7.3.0                                      \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe module `\u001b[32mnvidia\u001b[39m` is provided by more than one package, which causes an install race condition and can result in a broken module. Consider removing your dependency on either `\u001b[36mnvidia-nvjitlink-cu12\u001b[39m` (v\u001b[36m12.8.93\u001b[39m) or `\u001b[36mnvidia-cuda-cupti-cu12\u001b[39m` (v\u001b[36m12.8.90\u001b[39m).\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe module `\u001b[32mnvidia\u001b[39m` is provided by more than one package, which causes an install race condition and can result in a broken module. Consider removing your dependency on either `\u001b[36mnvidia-nvjitlink-cu12\u001b[39m` (v\u001b[36m12.8.93\u001b[39m) or `\u001b[36mnvidia-curand-cu12\u001b[39m` (v\u001b[36m10.3.9.90\u001b[39m).\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe module `\u001b[32mnvidia\u001b[39m` is provided by more than one package, which causes an install race condition and can result in a broken module. Consider removing your dependency on either `\u001b[36mnvidia-nvtx-cu12\u001b[39m` (v\u001b[36m12.8.90\u001b[39m) or `\u001b[36mnvidia-curand-cu12\u001b[39m` (v\u001b[36m10.3.9.90\u001b[39m).\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe module `\u001b[32mnvidia\u001b[39m` is provided by more than one package, which causes an install race condition and can result in a broken module. Consider removing your dependency on either `\u001b[36mnvidia-nvtx-cu12\u001b[39m` (v\u001b[36m12.8.90\u001b[39m) or `\u001b[36mnvidia-cusparse-cu12\u001b[39m` (v\u001b[36m12.5.8.93\u001b[39m).\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe module `\u001b[32mnvidia\u001b[39m` is provided by more than one package, which causes an install race condition and can result in a broken module. Consider removing your dependency on either `\u001b[36mnvidia-cusparse-cu12\u001b[39m` (v\u001b[36m12.5.8.93\u001b[39m) or `\u001b[36mnvidia-cuda-nvrtc-cu12\u001b[39m` (v\u001b[36m12.8.93\u001b[39m).\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe module `\u001b[32mnvidia\u001b[39m` is provided by more than one package, which causes an install race condition and can result in a broken module. Consider removing your dependency on either `\u001b[36mnvidia-cuda-runtime-cu12\u001b[39m` (v\u001b[36m12.8.90\u001b[39m) or `\u001b[36mnvidia-cuda-nvrtc-cu12\u001b[39m` (v\u001b[36m12.8.93\u001b[39m).\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe module `\u001b[32mnvidia\u001b[39m` is provided by more than one package, which causes an install race condition and can result in a broken module. Consider removing your dependency on either `\u001b[36mnvidia-cufile-cu12\u001b[39m` (v\u001b[36m1.13.1.3\u001b[39m) or `\u001b[36mnvidia-cuda-runtime-cu12\u001b[39m` (v\u001b[36m12.8.90\u001b[39m).\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe module `\u001b[32mnvidia\u001b[39m` is provided by more than one package, which causes an install race condition and can result in a broken module. Consider removing your dependency on either `\u001b[36mnvidia-cufile-cu12\u001b[39m` (v\u001b[36m1.13.1.3\u001b[39m) or `\u001b[36mnvidia-cublas-cu12\u001b[39m` (v\u001b[36m12.8.4.1\u001b[39m).\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe module `\u001b[32mnvidia\u001b[39m` is provided by more than one package, which causes an install race condition and can result in a broken module. Consider removing your dependency on either `\u001b[36mnvidia-cusolver-cu12\u001b[39m` (v\u001b[36m11.7.3.90\u001b[39m) or `\u001b[36mnvidia-cublas-cu12\u001b[39m` (v\u001b[36m12.8.4.1\u001b[39m).\u001b[0m\n",
      "\u001b[2K███░░░░░░░░░░░░░░░░░ [5/28] \u001b[2mfilelock==3.18.0                                    \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe module `\u001b[32mnvidia\u001b[39m` is provided by more than one package, which causes an install race condition and can result in a broken module. Consider removing your dependency on either `\u001b[36mnvidia-cusolver-cu12\u001b[39m` (v\u001b[36m11.7.3.90\u001b[39m) or `\u001b[36mnvidia-cufft-cu12\u001b[39m` (v\u001b[36m11.3.3.83\u001b[39m).\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m28 packages\u001b[0m \u001b[2min 1m 04s\u001b[0m\u001b[0m                             \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.18.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.1.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.34.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.99.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyngrok\u001b[0m\u001b[2m==7.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.4.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install --system huggingface_hub accelerate openai pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4df5542-8df7-4bba-bc63-43ce6e306226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.5 environment at: /opt/python\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m18 packages\u001b[0m \u001b[2min 6.47s\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 4.84s\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/2] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 517ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.0.dev0 (from git+https://github.com/huggingface/transformers.git@085e02383c3d74a4e18e7ef8404c281b3e8ccfb5)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install --system git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67ac027-523d-4c73-9804-62b89b20727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch:\n",
      "PyTorch is working with CUDA\n",
      "The GPU model is: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch:\")\n",
    "print(\"PyTorch is working with CUDA\" if torch.cuda.is_available() else \"Error! It is not working correctly\")\n",
    "print(\"The GPU model is: \"+ torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93465642-ceb5-42b5-9afe-2c9f3960aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ccf8a-53c7-4522-9e0e-953b52e9e6d9",
   "metadata": {},
   "source": [
    "# Run transformers serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95658b4e-f775-460b-b0eb-473e069e0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terminal 1\n",
    "!transformers serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae22a04-724e-4ca2-9ec9-668bc9f27403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 'list',\n",
       " 'data': [{'id': 'Menlo/Jan-nano',\n",
       "   'object': 'model',\n",
       "   'created': 1749539461.0,\n",
       "   'owned_by': 'Menlo'},\n",
       "  {'id': 'Menlo/Jan-nano-128k',\n",
       "   'object': 'model',\n",
       "   'created': 1750831938.0,\n",
       "   'owned_by': 'Menlo'},\n",
       "  {'id': 'Qwen/Qwen2.5-0.5B-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1726487566.0,\n",
       "   'owned_by': 'Qwen'},\n",
       "  {'id': 'Qwen/Qwen2.5-3B-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1726582132.0,\n",
       "   'owned_by': 'Qwen'},\n",
       "  {'id': 'Qwen/Qwen2.5-7B-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1726487740.0,\n",
       "   'owned_by': 'Qwen'},\n",
       "  {'id': 'Qwen/Qwen2.5-14B-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1726487770.0,\n",
       "   'owned_by': 'Qwen'},\n",
       "  {'id': 'meta-llama/Llama-3.1-8B-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1721292960.0,\n",
       "   'owned_by': 'meta-llama'},\n",
       "  {'id': 'meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1726672367.0,\n",
       "   'owned_by': 'meta-llama'},\n",
       "  {'id': 'meta-llama/Llama-3.3-70B-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1732637327.0,\n",
       "   'owned_by': 'meta-llama'},\n",
       "  {'id': 'HuggingFaceTB/SmolVLM-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1731948537.0,\n",
       "   'owned_by': 'HuggingFaceTB'},\n",
       "  {'id': 'ibm-granite/granite-vision-3.2-2b',\n",
       "   'object': 'model',\n",
       "   'created': 1739832624.0,\n",
       "   'owned_by': 'ibm-granite'},\n",
       "  {'id': 'Qwen/Qwen2.5-VL-7B-Instruct',\n",
       "   'object': 'model',\n",
       "   'created': 1737883597.0,\n",
       "   'owned_by': 'Qwen'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get(\"http://localhost:8000/v1/models\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7dbae13-b91b-4a34-b436-423c6c318091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MXFP4 quantization is a type of **quantization method** used in **machine learning**, particularly in **neural network training and inference**. It is a **post-training quantization** technique that reduces the precision of the model's weights and activations from **32-bit floating-point (FP32)** to **4-bit integers (INT4)**, which can significantly reduce the model's size and improve inference speed while maintaining acceptable accuracy.\n",
      "\n",
      "### Key Concepts:\n",
      "\n",
      "1. **Quantization**:\n",
      "   - Quantization is the process of reducing the precision of numerical values in a model to make it more efficient in terms of memory and computation.\n",
      "   - In machine learning, this is often done to enable **model compression**, **acceleration**, and **deployment on edge devices** (e.g., mobile phones, IoT devices).\n",
      "\n",
      "2. **MXFP4**:\n",
      "   - **MXFP4** stands for **Mixed-Exponent FP4** or **Mixed-Exponent 4-bit Floating Point**.\n",
      "   - It is a **4-bit floating-point format** that uses a **mixed exponent** scheme to represent numbers more efficiently than traditional 4-bit fixed-point formats.\n",
      "   - This format allows for a **wider dynamic range** compared to fixed-point quantization, which is important for maintaining numerical stability during inference.\n",
      "\n",
      "3. **Quantization Process**:\n",
      "   - **Training**: The model is trained in **FP32** precision.\n",
      "   - **Quantization**: After training, the model is quantized to **FP4** (or INT4, depending on the implementation).\n",
      "   - **Dequantization**: During inference, the model is dequantized back to FP32 to compute the output.\n",
      "\n",
      "4. **Benefits of MXFP4 Quantization**:\n",
      "   - **Reduced Model Size**: 4-bit weights take up less memory than 32-bit weights.\n",
      "   - **Faster Inference**: Lower precision operations are faster on hardware that supports them (e.g., GPUs, TPUs, and specialized AI accelerators).\n",
      "   - **Lower Power Consumption**: Reduced computation and memory usage lead to lower power consumption, which is important for edge devices.\n",
      "\n",
      "5. **Challenges**:\n",
      "   - **Accuracy Trade-off**: Quantization can introduce errors, especially if the model is not well-quantized.\n",
      "   - **Hardware Support**: MXFP4 requires hardware or software that supports 4-bit floating-point operations, which may not be available on all platforms.\n",
      "\n",
      "### Use Cases:\n",
      "- **Edge AI**: Deploying models on devices with limited computational resources.\n",
      "- **Mobile Applications**: Reducing model size for faster loading and execution on mobile devices.\n",
      "- **Real-Time Inference**: Enabling real-time processing on embedded systems.\n",
      "\n",
      "### Tools and Frameworks:\n",
      "- **TensorFlow Lite** and **PyTorch** support quantization, including post-training quantization.\n",
      "- **ONNX Runtime** and **TensorRT** also provide tools for quantizing models to 4-bit formats.\n",
      "\n",
      "### Summary:\n",
      "MXFP4 quantization is a **4-bit floating-point quantization method** that reduces the precision of neural network weights and activations, enabling **model compression**, **faster inference**, and **lower power consumption**. It is particularly useful for deploying machine learning models on **edge devices** and **resource-constrained environments**."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # Local Ollama API\n",
    "    api_key=\"dummy_string\"                       # Dummy key\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Menlo/Jan-nano\", #meta-llama/Llama-3.2-1B-Instruct\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    token = chunk.choices[0].delta.content\n",
    "    if token:\n",
    "        print(token, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69950f-d164-4d24-82c9-c57f258d9712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * ngrok tunnel \"https://333fecb39890.ngrok-free.app\" -> \"http://127.0.0.1:8000\"                    \n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok, conf\n",
    "conf.get_default().auth_token = \"...\"\n",
    "\n",
    "port = \"8000\"\n",
    "public_url = ngrok.connect(port).public_url\n",
    "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95d26d-dfd6-449b-b354-fbac8c0ed4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
